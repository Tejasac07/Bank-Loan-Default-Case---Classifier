rm(list = ls())
setwd("G:Tejas/R_practice/Project2_prac")
getwd()
x = c("ggplot2", "corrgram", "DMwR", "caret", "randomForest", "unbalanced", "C50", "dummies", "e1071", "Information",
"MASS", "rpart", "gbm", "ROSE", 'sampling', 'DataCombine', 'inTrees','fastDummies', 'psych')
#install.packages(x)
lapply(x, require, character.only = TRUE)
#load packages
library("dplyr")
library("plyr")
library("ggplot2")
library("data.table")
library("GGally")
library(tidyr)
#load data into r
loan_default = read.csv("bank_loan.csv", header = T, na.strings = c(" ","","NA"))
View(loan_default)
# Summarizing  data
# dim helps to see no of observations and variables in the dataset.
# dataset contains 850 obs. of 9 variables
dim(loan_default)
### TAble helps us to see how many for default and non default
table(loan_default$default)
### Unique Values ###
unique(loan_default$default)
head(loan_default)
tail(loan_default)
colnames(loan_default)
str(loan_default)
loan_default$default = as.factor(loan_default$default)
str(loan_default)
describe(loan_default)
############## Checking the distribution of the age ###
#### age is normally distributed.
plot(density(loan_default$age))
hist(loan_default$age, main = " age histogram " , xlab = 'age', ylab = "freq")
#####
ggplot(loan_default) +
geom_bar(aes(x=ed),fill="grey")
################ Missing Value Analysis ###################
sum(is.na(loan_default$default))
missing_val = data.frame(apply(loan_default, 2, function(x){sum(is.na(x))}))
missing_val
View(missing_val)
missing_val$columns = row.names(missing_val)
row.names(missing_val) = NULL
names(missing_val)[1] = "Count"
names(missing_val)[2] = "Variables"
missing_val$Missing_Percentage = (missing_val$Count/nrow(loan_default))*100
missing_val = missing_val[order(-missing_val$Missing_Percentage),]
missing_val = missing_val[, c(2, 1)]
missing_val$Missing_Percentage = (missing_val$Count/nrow(loan_default))*100
sum(is.na(loan_default))
#Write  output result into disk
write.csv(missing_val, "Missing_perc.csv", row.names = F)
##################### outlier check #####################
##### All the given variables has outliers. I assume that the income and related
# debt values are dependent on the observer. more the education more the values
### might be.
boxplot(loan_default$age, main = " outlier check of age variable", ylab = "age", col = 5)
boxplot(loan_default$income, main = " outlier check of income variable", ylab = "income", col = 5)
boxplot(loan_default$creddebt, main = " outlier check of creddebt variable", ylab = "creddebt", col = 5)
boxplot(loan_default$othdebt, main = " outlier check of othdebt variable", ylab = "othdebt", col = 5)
## Correlation Plot
numeric_index = sapply(loan_default, is.numeric)
corrgram(loan_default[,numeric_index], order = F,
upper.panel=panel.pie, text.panel=panel.txt, main = "Correlation Plot")
ggcorr(loan_default, label = T, label_size = 3, label_round = 2, hjust = 1, size = 3, color = "royalblue", layout.exp = 5, low = "dodgerblue", mid = "gray95", high = "red2", name = "Correlation Heatmap")
###### standardisation###########
loan_default_scaled = loan_default
cnames = colnames(loan_default_scaled)
for (i in cnames) {
print(i)
loan_default_scaled[,i] = (loan_default_scaled[,i]-mean(loan_default_scaled[,i]))/sd(loan_default_scaled[,i])
}
###################### separating the labeled and not labeled observations.
##### last 150 observations are not labled. Those observations to be
### predicted after the model building. we will predict those values after
### chossing best model.
loan_train = loan_default_scaled[1:700,1:9]
dim(loan_train)
sum(is.na(loan_train))
sum(is.na(loan_default$default))
loan_test  = loan_default_scaled[701:850, 1:8]
dim(loan_test)
############# Model Building##############
library(caTools)
#Splitting into training and testing data
set.seed(123)
sample = sample.split(loan_train, SplitRatio = 0.8)
sample
training = subset(loan_train, sample==TRUE)
str(training)
testing = subset(loan_train, sample==FALSE)
str(testing)
######################logistic regression #########################
model = glm(default~.,training, family = "binomial")
summary(model)
#########################model with high important variables############
model6 = glm(default~creddebt+debtinc+address+employ, training, family = "binomial")
summary(model6)
res = predict(model6, testing, type = "response")
range(res)
confusion_matrix = table(Actualvalue=testing$default, predictedvalue=res>0.5)
print(confusion_matrix)
accuracy = (104 + 20)/(104 + 20 + 24 + 7)
print(accuracy)
#Accuracy = 80%
###Threshold Evaluation ####
### ROC CURVE ####
###AUC####
library(ROCR)
pred_log = prediction(res, testing$default)
acc = performance(pred_log, "acc")
plot(acc)
roc_curve = performance(pred_log, "tpr" , "fpr")
plot(roc_curve)
plot(roc_curve , colorize = T, print.cutoffs.at=seq(0.1,by=0.1))
###### using threshold value of 0.4 we can incraese the true positive rate
confusion_matrix = table(Actualvalue = testing$default, predictedvalue = res>0.4)
print(confusion_matrix)
accuracy = (93 + 25)/(93 + 25 + 19 + 18)
print(accuracy)
auc = performance(pred_log, "auc")
auc
#accuracy = 76%
#AUC = 0.82
#############Precision recall curve ##############
library(PRROC)
PRC_curve = performance(pred_log, "prec" , "rec")
plot(PRC_curve, colorize = T)
############################## DEcision tree###############
library(tree)
deci_model = tree(default~., data = training)
summary(deci_model)
### plotting
plot(deci_model)
text(deci_model,pretty = 0)
#### prediction
deci_pred = predict(deci_model, testing, type = "class")
confusion_matrix = table(Actualvalue = testing$default, predictedvalue = deci_pred)
print(confusion_matrix)
accuracy = (96 + 19)/(96 + 19 + 25 + 15)
accuracy
#### cross validation
cv.deci_model = cv.tree(deci_model, FUN = prune.misclass)
cv.deci_model
plot(cv.deci_model)
####pruning
prune.deci_model = prune.misclass(deci_model, best = 10)
plot(prune.deci_model)
text(prune.deci_model)
#### prediction of values again
deci_predict_1 = predict(prune.deci_model, testing, type = "class")
Confusion_matrix_1 = table(testing$default, deci_predict_1)
print(Confusion_matrix_1)
accuracy = (96 + 19)/(96 + 19 + 25 + 15)
accuracy
# accuracy = 0.74 # precision = 0.54 # recall = 0.43
##############################Random Forest#################
#####random forest 1
library(randomForest)
rf = randomForest(default~., data = training)
print(rf)
## prediction
rf_predict = predict(rf,testing)
confusion_matrix = table(Actualvalue = testing$default, predictedvalue = rf_predict)
print(confusion_matrix)
accuracy = (101 + 17)/(101 + 17 + 27 + 10)
print(accuracy)
## tune mtry
tuneRF(training[,-9], training[,9],stepfactor = 0.5,
plot = TRUE , ntreeTry = 1000,
trace = TRUE ,
improve = 0.05)
rf1 = randomForest(default~.,data = training, ntree = 1000, mtry = 2)
rf1
# predict
rf_predict1 = predict(rf1,testing)
confusion_matrix1 = table(Actualvalue=testing$default, predictedvalue=rf_predict1)
print(confusion_matrix1)
accuracy = (101 + 18)/(101 + 18 + 26 + 10)
print(accuracy)
# no. of nodes for the trees
hist(treesize(rf1),main = " no. of nodes for the trees", col = "green")
# variable importance
varImpPlot(rf1,
sort = T,
main = "variable importance")
importance(rf1)
varUsed(rf1)
############## we will build random forest by taking only max meandecreaseGini
### considering debtinc, employ, creddebt, othdeb, income.
### build model
rf_final = randomForest(default~debtinc+employ+creddebt+othdebt+income ,
data = training,
ntree = 1000, mtry = 2)
rf_final
# prediction
rf_predict_final = predict(rf_final,testing)
confusion_matrix_f = table(Actualvalue=testing$default, predictedvalue=rf_predict_final)
print(confusion_matrix_f)
accuracy = (100 + 14)/(100 + 14 + 30 + 11)
print(accuracy)
# accuracy = 73.5
# precision = 0.55
# recall = 0.340
#### we can not decide the perfomance of model only based on the accuracy
# we need to have a good trade off between precision and recall.
# logistic model has 80.0 % accuracy with good trade-off b/t prec and recall.
##### Conclusion ======= logistic model is the best suited model on this dataset.
#### predicting for the test data.
res = predict(model6, loan_test, type = "response")
range(res)
savehistory(file = "final_code.Rhistory")
